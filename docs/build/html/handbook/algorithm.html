<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>6. 多智能体强化学习算法介绍 &mdash; AMB v1.0.0 文档</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=12f31cd9"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="../_static/translations.js?v=beaddf03"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="7. 对抗攻击算法介绍" href="adversarial.html" />
    <link rel="prev" title="5. 环境介绍" href="environment.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            AMB
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="搜索文档" aria-label="搜索文档" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="导航菜单">
              <p class="caption" role="heading"><span class="caption-text">文档目录</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">1. 平台简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="introduction.html#id4">2. 项目成员</a></li>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">3. 项目架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">4. 快速开始</a></li>
<li class="toctree-l1"><a class="reference internal" href="environment.html">5. 环境介绍</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">6. 多智能体强化学习算法介绍</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#ippo-independent-proximal-policy-optimization">6.1. IPPO(Independent Proximal Policy Optimization)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id2">6.1.1. 概念</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id3">6.1.2. 训练流程</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id4">6.1.3. 形式化定义</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#mappo-multi-agent-proximal-policy-optimization">6.2. MAPPO(Multi-Agent Proximal Policy Optimization)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id5">6.2.1. 概念</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id6">6.2.2. 训练流程</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id7">6.2.3. 形式化定义</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#maddpg-multi-agent-deep-deterministic-policy-gradient">6.3. MADDPG(Multi-agent deep deterministic policy gradient)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id8">6.3.1. 概念</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id9">6.3.2. 形式化定义</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#iql-independent-q-learning">6.4. IQL(Independent Q learning)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id10">6.4.1. 概念</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id11">6.4.2. 训练流程</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id12">6.4.3. 形式化定义</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#vdn-value-decomposition-networks">6.5. VDN(Value_Decomposition NetWorks)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id13">6.5.1. 概念</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id14">6.5.2. 形式化定义</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#qmix">6.6. QMIX</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id16">6.6.1. 概念</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id17">6.6.2. 形式化定义</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#happo-heterogeneous-agent-proximal-policy-optimization">6.7. HAPPO(Heterogeneous-Agent Proximal Policy Optimization)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id18">6.7.1. 概念</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id19">6.7.2. 形式化定义</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="adversarial.html">7. 对抗攻击算法介绍</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="移动版导航菜单" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">AMB</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="页面导航">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active"><span class="section-number">6. </span>多智能体强化学习算法介绍</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/handbook/algorithm.rst.txt" rel="nofollow"> 查看页面源码</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="id1">
<h1><span class="section-number">6. </span>多智能体强化学习算法介绍<a class="headerlink" href="#id1" title="Link to this heading"></a></h1>
<p>多智能体算法比单智能体算法更加复杂，因为每个智能体在和环境交互的同时，也在和其他智能体进行直接或者间接的交互。因此多智能体强化学习算法比单智能体强化学习算法更加困难，其难点主要体现为：</p>
<blockquote>
<div><ul class="simple">
<li><p>由于多个智能体在环境中进行实时动态交互，并且每个智能体都在不断学习并且更新自身策略，因此在每个智能体的视角下，环境是非稳态的，及对于一个智能体而言，即使在相同的状态下采取相同的动作，得到的状态转移和奖励的分布也可能在不断变化。</p></li>
<li><p>多个智能体的训练可能是多目标的，不同智能体需要最大化自己的利益。</p></li>
<li><p>训练评估的复杂度会增加，可能需要大规模分布式训练来提高效率。</p></li>
</ul>
</div></blockquote>
<section id="ippo-independent-proximal-policy-optimization">
<span id="ippo"></span><h2><span class="section-number">6.1. </span>IPPO(Independent Proximal Policy Optimization)<a class="headerlink" href="#ippo-independent-proximal-policy-optimization" title="Link to this heading"></a></h2>
<section id="id2">
<h3><span class="section-number">6.1.1. </span>概念<a class="headerlink" href="#id2" title="Link to this heading"></a></h3>
<p>IPPO（Independent Proximal Policy Optimization）是一种完全去中心化（每个Agent独立和环境交互并用自己的观测和奖励来更新自己的策略）的算法，直接将PPO算法应用到多智能体环境中，即对多智能体环境中的每个智能体使用单智能体算法PPO进行训练。</p>
<img alt="../_images/ippo.png" src="../_images/ippo.png" />
</section>
<section id="id3">
<h3><span class="section-number">6.1.2. </span>训练流程<a class="headerlink" href="#id3" title="Link to this heading"></a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>对于N个智能体，为每个智能体初始化各自的策略以及价值函数
for 训练轮数 k=0,1,2,... do
    所有智能体在环境中交互分别获得各自的一条轨迹数据
    对每个智能体，基于当前的价值函数用GAE计算优势函数的估计
    对于每个智能体，通过最大化其PPO-clip截断的目标来更新策略
    对于每个智能体，通过均方差误差损失函数优化其价值函数
end for
</pre></div>
</div>
</section>
<section id="id4">
<h3><span class="section-number">6.1.3. </span>形式化定义<a class="headerlink" href="#id4" title="Link to this heading"></a></h3>
<p><span class="math notranslate nohighlight">\(GAE\)</span> 的思想是从对第 1 步到 n 步计算出的各个优势函数进行加权，加权计算出的均值为最后的优势函数，这样做可以减少方差。</p>
<div class="math notranslate nohighlight">
\[\begin{split}A_t^a = \sum_{l=0}^{h} (\gamma \lambda)^l \delta_{t+l}^{sa}\\
\delta_t^a = r_t(z_t^a, u_t^a) + \gamma V_{\phi}(z_{t+1}^a) - V_{\phi}(z_t^a)\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(actor\)</span> 目标函数为</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}^a(\theta) = \mathbb{E}_{z_t^a, u_t^a} \left[ \min \left( \frac{\pi_{\theta}(u_t^a|z_t^a)}{\pi_{\theta_{old}}(u_t^a|z_t^a)} A_t^a, \text{clip}\left( \frac{\pi_{\theta}(u_t^a|z_t^a)}{\pi_{\theta_{old}}(u_t^a|z_t^a)}, 1-\epsilon, 1+\epsilon \right) A_t^a \right) \right]\]</div>
<p><span class="math notranslate nohighlight">\(critic\)</span> 目标函数为</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}^a(\phi) = \mathbb{E}_{z_t^a} \left[ \min \left( \left(V_{\phi}(z_t^a) - \hat{V}_t^a \right)^2, \left(V_{\phi_{old}}(z_t^a) + \text{clip}\left(V_{\phi}(z_t^a) - V_{\phi_{old}}(z_t^a), -\epsilon, +\epsilon\right) - \hat{V}_t^a \right)^2 \right) \right]\]</div>
<div class="admonition hint">
<p class="admonition-title">提示</p>
<p>实际上，缓冲区和智能体模型可以在不同的智能体之间选择是否共享，这适用于全部 PPO 类算法。</p>
</div>
</section>
</section>
<section id="mappo-multi-agent-proximal-policy-optimization">
<span id="mappo"></span><h2><span class="section-number">6.2. </span>MAPPO(Multi-Agent Proximal Policy Optimization)<a class="headerlink" href="#mappo-multi-agent-proximal-policy-optimization" title="Link to this heading"></a></h2>
<section id="id5">
<h3><span class="section-number">6.2.1. </span>概念<a class="headerlink" href="#id5" title="Link to this heading"></a></h3>
<p>MAPPO(Multi-Agent Proximal Policy Optimization)是基于PPO的多智能体版本，专门为多智能体环境设计。
MAPPO 继承了 IPPO 的核心特性，同时加了额外的机制比如一个中心化的critic函数，来处理多智能体之间的交互和协调。</p>
<p>相对于IPPO，MAPPO的主要改进是：</p>
<ul class="simple">
<li><p><strong>中心化训练与去中心化执行</strong>：  MAPPO 通常使用中心化训练和去中心化执行框架（Centralized Training with Decentralized Execution, CDTE）框架。在此框架中，训练阶段利用利用所有智能体的信息来学习策略，但是在执行阶段，每个智能体仅使用自己的局部观测来制定决策，满足实时性和可扩展性的需求。</p></li>
<li><p><strong>应对非静态环境</strong>：  MAPPO 通过集中式训练可以更好地应对环境中的非静态性，因为所有智能体的策略更新都考虑了其他智能体的潜在行动和策略。这种方法能够促进更复杂的协同和竞争策略的形成。</p></li>
<li><p><strong>学习效率和复杂环境的适应性</strong>： 通过中央控制器的辅助，MAPPO能够在更复杂的多智能体环境中实现更高效的学习。集中处理所有智能体的信息可以帮助算法更快地识别有效的协作或对抗模式，从而加速学习过程。</p></li>
</ul>
<img alt="../_images/mappo.png" src="../_images/mappo.png" />
</section>
<section id="id6">
<h3><span class="section-number">6.2.2. </span>训练流程<a class="headerlink" href="#id6" title="Link to this heading"></a></h3>
<p>训练流程和上述 IPPO 相同。</p>
</section>
<section id="id7">
<h3><span class="section-number">6.2.3. </span>形式化定义<a class="headerlink" href="#id7" title="Link to this heading"></a></h3>
<p><span class="math notranslate nohighlight">\(Critic\)</span> 更新：每次迭代给出一个更好的中心化的价值函数。</p>
<div class="math notranslate nohighlight">
\[{\phi}_{k+1}=\arg \max_{\phi} \frac{1}{|D_k|\,T} \sum_{\tau \in D_k}\sum_{t=0}^{T} \left( V_{\left( \phi \right)}\left( o_t, s_t, u_t \right) - \hat{R}_t \right)^2\]</div>
<p><span class="math notranslate nohighlight">\(GAE\)</span> 函数： 给出当前动作相对于 <span class="math notranslate nohighlight">\(critic value\)</span> 的好坏程度。</p>
<div class="math notranslate nohighlight">
\[\begin{split}A_t^a = \sum_{l=0}^{h} (\gamma \lambda)^l \delta_{t+l}^{sa}\\
\delta_t^a = r_t(z_t^a, u_t^a) + \gamma V_{\phi}(z_{t+1}^a) - V_{\phi}(z_t^a)\end{split}\]</div>
<p>策略学习的损失函数：使用优势估计就散策略梯度来更新策略函数。</p>
<div class="math notranslate nohighlight">
\[L(o, s, u, u', \theta_k, \theta) = \min \left( \frac{\pi_\theta(u|o)}{\pi_{\theta_k}(u|o)} A^{\theta_k}(o, s, u'), \text{clip}\left( \frac{\pi_\theta(u|o)}{\pi_{\theta_k}(u|o)}, 1-\epsilon, 1+\epsilon \right) A^{\theta_k}(o, s, u') \right)\]</div>
<p><span class="math notranslate nohighlight">\(D\)</span> 是收集到的可以在代理之间共享的轨迹。 <span class="math notranslate nohighlight">\(R\)</span> 是奖励。 <span class="math notranslate nohighlight">\(\tau\)</span> 是轨迹。 <span class="math notranslate nohighlight">\(A\)</span> 是优势。 <span class="math notranslate nohighlight">\(\gamma\)</span> 是折扣值。
<span class="math notranslate nohighlight">\(\lambda\)</span> 是GAE的权重值。 <span class="math notranslate nohighlight">\(u\)</span> 是当前代理操作。 <span class="math notranslate nohighlight">\(u'\)</span> 是除当前代理之外的所有代理的操作集。
<span class="math notranslate nohighlight">\(s\)</span> 是全局状态。 <span class="math notranslate nohighlight">\(o\)</span> 是局部观察
<span class="math notranslate nohighlight">\(\epsilon\)</span> 是一个超参数，控制新策略允许与旧策略相差多远。
<span class="math notranslate nohighlight">\(V_{\phi}\)</span> 是价值函数，可以在代理之间共享。
<span class="math notranslate nohighlight">\(\pi_{\theta}\)</span> 是策略函数，可以在代理之间共享。</p>
</section>
</section>
<section id="maddpg-multi-agent-deep-deterministic-policy-gradient">
<span id="maddpg"></span><h2><span class="section-number">6.3. </span>MADDPG(Multi-agent deep deterministic policy gradient)<a class="headerlink" href="#maddpg-multi-agent-deep-deterministic-policy-gradient" title="Link to this heading"></a></h2>
<section id="id8">
<h3><span class="section-number">6.3.1. </span>概念<a class="headerlink" href="#id8" title="Link to this heading"></a></h3>
<p>MADDPG(Multi-agent deep deterministic policy gradient) 算法是 DDPG(Multi-agent deep deterministic policy gradient) 算法对于多智能体的延申。每一个智能体均基于全局的 Q 函数以学习自身的策略。
在采样阶段， MADDPG 中的每哥智能体都遵循相同的 DDPG 算法来推断动作。不过每个智能体不是根据自己的动作计算 Q 值，而是使用集中式 Q 函数，该函数将所有智能体的动作作为输入来计算 Q 值。这需要在将数据存储到缓冲区之前在智能体之间共享数据。
在学习阶段，每个智能体使用目标策略预测其下一步行动，并在进入训练循环之前与其他智能体共享。这样做是为了确保所有代理在下一个采样阶段使用相同的操作来计算集中式 Q 函数中的 Q 值。</p>
<img alt="../_images/maddpg.png" src="../_images/maddpg.png" />
</section>
<section id="id9">
<h3><span class="section-number">6.3.2. </span>形式化定义<a class="headerlink" href="#id9" title="Link to this heading"></a></h3>
<p><span class="math notranslate nohighlight">\(Q-learning\)</span> ： 获得更好的集中式的Q函数。</p>
<div class="math notranslate nohighlight">
\[L(\phi, D) = \mathbb{E}_{(o,s,u,r,o',s',d) \sim D} \left[ \left( Q_\phi(o, s, u, r, o', s', d) - \left( r + \gamma (1 - d) Q_{\phi_{\text{targ}}}(o', s', \mu_{\phi_{\text{targ}}}(o')) \right) \right)^2 \right]\]</div>
<p>策略更新： 通过更新策略函数来最大化 <span class="math notranslate nohighlight">\(Q-Function\)</span> 输出。</p>
<div class="math notranslate nohighlight">
\[\max_\theta \mathbb{E}_{o,s \sim D} \left[ Q_{\phi}(o, s, \mu_\theta(o)) \right]\]</div>
<p><span class="math notranslate nohighlight">\(D\)</span> 是重播缓冲区，可以在代理之间共享。 <span class="math notranslate nohighlight">\(u\)</span> 是一个动作集，包括对手。 <span class="math notranslate nohighlight">\(r\)</span> 是奖励。 <span class="math notranslate nohighlight">\(s\)</span> 是观察/状态集，包括对手。
<span class="math notranslate nohighlight">\(s'\)</span> 是下一个观察/状态集，包括对手。 <span class="math notranslate nohighlight">\(d\)</span> 当剧集结束时设置为 1（真），否则设置为 0（假）。
<span class="math notranslate nohighlight">\(\gamma\)</span> 是折扣值。 <span class="math notranslate nohighlight">\(\mu_{\theta}\)</span> 是一个可以在代理之间共享的策略函数。
<span class="math notranslate nohighlight">\(Q_{\phi}\)</span> 是Q函数，可以跨agent共享。 <span class="math notranslate nohighlight">\(\mu_{\theta_{targ}}\)</span> 是目标策略函数，可以在代理之间共享。
<span class="math notranslate nohighlight">\(Q_{\theta_{targ}}\)</span> 是目标 Q 函数，可以在代理之间共享。</p>
</section>
</section>
<section id="iql-independent-q-learning">
<span id="iql"></span><h2><span class="section-number">6.4. </span>IQL(Independent Q learning)<a class="headerlink" href="#iql-independent-q-learning" title="Link to this heading"></a></h2>
<section id="id10">
<h3><span class="section-number">6.4.1. </span>概念<a class="headerlink" href="#id10" title="Link to this heading"></a></h3>
<p>IQL(Independent Q learning) 是 Q-learning 算法在多智能体环境下的自然拓展。
每个智能体利用自己 <cite>buffer</cite> 收集的数据训练一个标准DQN策略网络。这意味着每个智能体独立拥有属于自己的Q-Function而不是和其他智能体共享信息，不过IQL中也可以实现跨智能体的信息共享。</p>
<img alt="../_images/iql.png" src="../_images/iql.png" />
</section>
<section id="id11">
<h3><span class="section-number">6.4.2. </span>训练流程<a class="headerlink" href="#id11" title="Link to this heading"></a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>对于N个智能体，为每个智能体初始化各自的Q函数和目标Q函数
for 训练轮数 k=0,1,2,... do
    所有智能体在环境中交互
    对每个智能体，基于当前时刻和下一时刻的观测和动作对当前动作的计算Q值和进行目标Q值估计
    最小化当前Q值和目标Q值的差距，从而优化更好的策略
end for
</pre></div>
</div>
</section>
<section id="id12">
<h3><span class="section-number">6.4.3. </span>形式化定义<a class="headerlink" href="#id12" title="Link to this heading"></a></h3>
<p>Q 值更新的方式如下：</p>
<div class="math notranslate nohighlight">
\[Q(s, a) = (1 - \alpha) Q(s, a) + \alpha (r + \gamma \max_{a'} Q(s', a'))\]</div>
<p>目标是最小化预测 Q 值与从贝尔曼方程获得的目标 Q 值之间的差异。</p>
<div class="math notranslate nohighlight">
\[\phi_{k+1} = \arg\min_\phi \left( Q_\phi(s, a) - (r + \lambda \cdot \max_{a'} Q_{\phi_{\text{tar}}}(s', a')) \right)^2\]</div>
<div class="admonition hint">
<p class="admonition-title">提示</p>
<p>在多智能体学习领域，“信息共享”一词可能含糊不清，因此有必要在此进行澄清。信息共享可分为三种类型：</p>
<ul class="simple">
<li><p>真实/采样数据：观察、行动等。</p></li>
<li><p>预测数据：Q/临界值、通信消息等。</p></li>
<li><p>知识：经验回放缓冲区、模型参数等。</p></li>
</ul>
<p>IQL 可对经验回放缓冲区、模型参数等知识进行共享。</p>
</div>
</section>
</section>
<section id="vdn-value-decomposition-networks">
<span id="vdn"></span><h2><span class="section-number">6.5. </span>VDN(Value_Decomposition NetWorks)<a class="headerlink" href="#vdn-value-decomposition-networks" title="Link to this heading"></a></h2>
<section id="id13">
<h3><span class="section-number">6.5.1. </span>概念<a class="headerlink" href="#id13" title="Link to this heading"></a></h3>
<p>VDN(Value_Decomposition NetWorks) 遵循和其他 Q-learning 算法相同的数据采样方式，在进入训练之前，每个智能体和其他智能体分享其 <span class="math notranslate nohighlight">\(Q\)</span> 值和目标 <span class="math notranslate nohighlight">\(Q\)</span> 值。
在训练的过程中，将当前智能体和其他智能体的 <span class="math notranslate nohighlight">\(Q\)</span> 值和目标 <span class="math notranslate nohighlight">\(Q\)</span> 值相加，得到 <span class="math notranslate nohighlight">\(Q_{total}\)</span> 值，使得每个智能体能够结合其他智能体的行为对环境的影响，从而做出更加明智的决策。</p>
<img alt="../_images/vdn.png" src="../_images/vdn.png" />
</section>
<section id="id14">
<h3><span class="section-number">6.5.2. </span>形式化定义<a class="headerlink" href="#id14" title="Link to this heading"></a></h3>
<p><span class="math notranslate nohighlight">\(Q_{total}\)</span> ：将全部智能体的 <span class="math notranslate nohighlight">\(Q\)</span> 值来计算全局 <span class="math notranslate nohighlight">\(Q\)</span> 值。</p>
<div class="math notranslate nohighlight">
\[Q_{\phi}^{tot}=\sum_{l=0}^{n} Q_{\phi}^i\]</div>
<p><span class="math notranslate nohighlight">\(Q-Function\)</span> ：在每次迭代中获得对对全局 <span class="math notranslate nohighlight">\(Q\)</span> 值的估计，并且将梯度传递给每个智能体的 <span class="math notranslate nohighlight">\(Q\)</span> 值网络以完成更新。</p>
<div class="math notranslate nohighlight">
\[L(\phi, D) = \mathbb{E}_{\tau \sim D} \left[ Q^{tot}_\phi - \left( r + \gamma (1 - d) Q^{tot'}_{\phi_{targ}} \right)^2 \right]\]</div>
<div class="admonition hint">
<p class="admonition-title">提示</p>
<p>VDN 通过简单将所有奖励相加的操作来优化多个代理的联合策略。
然而，此操作减少了策略的表示，因为提取的去中心化策略不需要完全分解来与中心化策略完全一致。
即VDN强制每个智能体寻找最佳动作来满足以下方程：</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{equation}
\arg\max_{u} Q_{\text{tot}}(\tau, u) =
\left\{
\begin{array}{l}
\arg\max_{u^1} Q_1(\tau^1, u^1) \\
\vdots \\
\arg\max_{u^n} Q_n(\tau^n, u^n)
\end{array}
\right.
\end{equation}\end{split}\]</div>
</div>
</section>
</section>
<section id="qmix">
<span id="id15"></span><h2><span class="section-number">6.6. </span>QMIX<a class="headerlink" href="#qmix" title="Link to this heading"></a></h2>
<section id="id16">
<h3><span class="section-number">6.6.1. </span>概念<a class="headerlink" href="#id16" title="Link to this heading"></a></h3>
<p>与 VDN(Value_Decomposition NetWorks) 简单对全部智能体的 Q 值进行相加不同， QMIX 使用了一个前馈神经网络 Mixer ，将不同智能体的策略网络输出作为输入，并且将它们单调地混合。
其中施加得的约束方程如下:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\frac{\partial Q_{tot}}{\partial Q_a} \geq 0, \forall a \in A\]</div>
</div></blockquote>
<p>为了满足单调约束，混合网络的权重被限制为非负数。</p>
<img alt="../_images/qmix.png" src="../_images/qmix.png" />
</section>
<section id="id17">
<h3><span class="section-number">6.6.2. </span>形式化定义<a class="headerlink" href="#id17" title="Link to this heading"></a></h3>
<p>QMIX 需要不同的智能体共享信息，此处使用加粗的符号来标识包含多个智能体的信息。</p>
<p><span class="math notranslate nohighlight">\(Q-mix\)</span> ：基于前馈神经网络实现的混合器，通过 <span class="math notranslate nohighlight">\(Q-mix\)</span> 混合全部 Q 值来计算全局 Q 值。</p>
<div class="math notranslate nohighlight">
\[\mathbf{Q}_{total}(\mathbf{a},\mathbf{s};\phi,\psi)=g_{\psi}(\mathbf{s},Q_{\phi_1},Q_{\phi_2},...,Q_{\phi_n})\]</div>
<p><span class="math notranslate nohighlight">\(Q-Function\)</span> ：在每次迭代中获得对对全局 <span class="math notranslate nohighlight">\(Q\)</span> 值的估计，并且将梯度传递给混合器和每个智能体的 <span class="math notranslate nohighlight">\(Q\)</span> 值网络以完成更新。</p>
<div class="math notranslate nohighlight">
\[L(\phi, D) = \mathbb{E}_{\tau \sim D} \left[ Q^{tot}_\phi - \left( r + \gamma (1 - d) Q^{tot'}_{\phi_{targ}} \right)^2 \right]\]</div>
</section>
</section>
<section id="happo-heterogeneous-agent-proximal-policy-optimization">
<span id="happo"></span><h2><span class="section-number">6.7. </span>HAPPO(Heterogeneous-Agent Proximal Policy Optimization)<a class="headerlink" href="#happo-heterogeneous-agent-proximal-policy-optimization" title="Link to this heading"></a></h2>
<section id="id18">
<h3><span class="section-number">6.7.1. </span>概念<a class="headerlink" href="#id18" title="Link to this heading"></a></h3>
<p>HAPPO（异构近端体策略优化）是一种先进的多智能体强化学习算法，它在各种多智能体环境中表现出色，特别是在处理异质智能体的情况下。在多智能体MuJoCo环境中，HAPPO相比于传统的MAPPO算法展现了显著的优势，刷新了在策略上的最佳结果。在包含高达17个智能体的Humanoid控制任务中，HAPPO算法达到了行业领先水平，而MAPPO在这种高度异质的智能体任务中表现不佳，这凸显了HAPPO在促进异质智能体合作方面的优越性。总体而言，HAPPO是一种有效的多智能体强化学习算法，特别适合于处理具有高度异质性和复杂协作需求的智能体环境。</p>
</section>
<section id="id19">
<h3><span class="section-number">6.7.2. </span>形式化定义<a class="headerlink" href="#id19" title="Link to this heading"></a></h3>
<p>HAPPO在算法逻辑结构上与MAPPO类似，主要差别在于HAPPO会在更新完每个Agent策略后还要更新一下 <code class="docutils literal notranslate"><span class="pre">factor</span></code>，而该 <code class="docutils literal notranslate"><span class="pre">factor</span></code> 的作用大致是平衡旧策略和新策略。其作用流程是，在训练前后分别计算 <code class="docutils literal notranslate"><span class="pre">logprob</span></code> → 计算 <code class="docutils literal notranslate"><span class="pre">factor</span></code> → 将 <code class="docutils literal notranslate"><span class="pre">factor</span></code> 更新到 <code class="docutils literal notranslate"><span class="pre">buffer</span></code> 中 → <code class="docutils literal notranslate"><span class="pre">buffer</span></code> 的数据用来训练，形成闭环。</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}_{\mathrm{s} \sim \rho_{\boldsymbol{\pi}_{\boldsymbol{k}}}, \mathbf{a}^{\sim} \pi_{\boldsymbol{\theta}_{\boldsymbol{k}}}}\left[\min \left(\frac{\pi_{\theta^{i m}}^{i_m}\left(\mathrm{a}^i \mid \mathrm{s}\right)}{\pi_{\theta_k^{i_m}}^{i_m}\left(\mathrm{a}^i \mid \mathrm{s}\right)} M^{i_{1: m}}(\mathrm{~s}, \mathbf{a}), \operatorname{clip}\left(\frac{\pi_{\theta^{i m}}^{i_m}\left(\mathrm{a}^i \mid \mathrm{s}\right)}{\pi_{\theta_k^{i_m}}\left(\mathrm{a}^i \mid \mathrm{s}\right)}, 1 \pm \epsilon\right) M^{i_{1: m}}(\mathrm{~s}, \mathbf{a})\right)\right]\]</div>
<p>其中， <code class="docutils literal notranslate"><span class="pre">factor</span></code> 的定义如下：</p>
<div class="math notranslate nohighlight">
\[M^{i_{1: m}}=\frac{\overline{\boldsymbol{\pi}}^{i_{1: m-1}-1}\left(\mathbf{a}^{i_{1: m-1}} \mid s\right)}{\boldsymbol{\pi}^{i_{1: m-1}}\left(\mathbf{a}^{i_{1: m-1}} \mid s\right)} \hat{A}(s, \mathbf{a})\]</div>
<dl class="simple">
<dt>相关链接：</dt><dd><ul class="simple">
<li><p><a class="reference external" href="https://github.com/PKU-MARL/HARL">HAPPO</a></p></li>
</ul>
</dd>
</dl>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="页脚">
        <a href="environment.html" class="btn btn-neutral float-left" title="5. 环境介绍" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
        <a href="adversarial.html" class="btn btn-neutral float-right" title="7. 对抗攻击算法介绍" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 2023, BUAA NLSDE。</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用的 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a> 开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>